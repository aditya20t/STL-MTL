{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the required libraries\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose Dataset\n",
    "# StereoSet\n",
    "# ToxicBias\n",
    "dataset = 'StereoSet'\n",
    "\n",
    "#Choose encoder model\n",
    "# bert-base-uncased\n",
    "\n",
    "model = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size:  12226\n",
      "Validation size:  1360\n",
      "Test size:  3398\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(f'Dataset/{dataset}/train.csv')\n",
    "df_val = pd.read_csv(f'Dataset/{dataset}/val.csv')\n",
    "df_test = pd.read_csv(f'Dataset/{dataset}/test.csv')\n",
    "print('Training size: ', df_train.size)\n",
    "print('Validation size: ', df_val.size)\n",
    "print('Test size: ', df_test.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of sentences and labels\n",
    "sentences_train = df_train.Sentence.values\n",
    "labels_train = df_train.labels.values\n",
    "sentences_val = df_val.Sentence.values\n",
    "labels_val = df_val.labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  The schoolboy has some young qualities about him. Young qualities are usually producing charming people.\n",
      "Tokenized:  ['the', 'school', '##boy', 'has', 'some', 'young', 'qualities', 'about', 'him', '.', 'young', 'qualities', 'are', 'usually', 'producing', 'charming', 'people', '.']\n",
      "Token IDs:  [1996, 2082, 11097, 2038, 2070, 2402, 11647, 2055, 2032, 1012, 2402, 11647, 2024, 2788, 5155, 11951, 2111, 1012]\n"
     ]
    }
   ],
   "source": [
    "# Print the original sentence.\n",
    "print(' Original: ', sentences_train[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences_train[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences_train[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that encode every sentence, add padding and return the input ids and attention mask\n",
    "def encode_sentences(sentences):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "\n",
    "    # For every sentence\n",
    "    for sent in sentences:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sent,\n",
    "            add_special_tokens=True,\n",
    "            max_length=64,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        #Add the encoded sentence to list\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        #Add attention mask\n",
    "        attention_mask.append(encoded_dict['attention_mask'])\n",
    "    return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/u/adityatomar/miniforge3/envs/lmharness/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  The schoolboy has some young qualities about him. Young qualities are usually producing charming people.\n",
      "Token ID:  tensor([  101,  1996,  2082, 11097,  2038,  2070,  2402, 11647,  2055,  2032,\n",
      "         1012,  2402, 11647,  2024,  2788,  5155, 11951,  2111,  1012,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "# Get input ids and attention mask\n",
    "train_input_ids, train_attention_mask = encode_sentences(sentences_train)\n",
    "val_input_ids, val_attention_mask = encode_sentences(sentences_val)\n",
    "\n",
    "#Convert the list into tensors\n",
    "train_input_ids = torch.cat(train_input_ids, dim=0)\n",
    "train_attention_mask = torch.cat(train_attention_mask, dim=0)\n",
    "labels_train = torch.tensor(labels_train)\n",
    "val_input_ids = torch.cat(val_input_ids, dim=0)\n",
    "val_attention_mask = torch.cat(val_attention_mask, dim=0)\n",
    "labels_val = torch.tensor(labels_val)\n",
    "\n",
    "#Print sentence 0 \n",
    "print('Original: ', sentences_train[0])\n",
    "print('Token ID: ', train_input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmharness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
